{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy seismology: Digitizing old waveforms for modern seismology\n",
    "## IASPEI Early Career Scientific School on digitizing legacy seismic data and validating them using ocean microseism\n",
    "\n",
    "## Authors: \n",
    "* Raphael De Plaen ([@RDePlaen](https://github.com/RDePlaen))\n",
    "* Thomas Lecocq ([@seismotom](https://github.com/ThomasLecocq))\n",
    "* Josep Batllo\n",
    "\n",
    "## Introduction:\n",
    "\n",
    "Analog seismograms on photographic paper, smoked paper and other magnetic support are records of ground motion that cover most of the 20th century. Whether for past earthquakes or the analysis of continuous wavefiled for time dependent seismology, this represents a wealth of information that can not be found anywhere else but remains hard to exploit with modern tools.\n",
    "In this excersise, we will explore the digitisation of old seismic records and the challenges that can be encountered before they can be used for modern seismology.\n",
    "The paper seismograms that we will use have already been scanned but need to be vectorised.\n",
    "\n",
    "Throughout much of the 20th century, ground motion was recorded on analog seismograms using photographic paper, smoked paper, magnetic media, etc. \n",
    "These records capture an invaluable archive of past earthquakes and continuous seismic signals, offering insights that are unavailable from modern datasets alone, seismic or otherwise. \n",
    "Making use of this information today is challenging, as the original formats are not directly compatible with modern analysis tools.\n",
    "\n",
    "In this exercise, we will explore the process of digitizing historical seismic records and discuss the main challenges involved in preparing them for contemporary seismological studies. The seismograms we will work with have already been scanned, but they still need to be converted into vectorized form before they can be analyzed.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./figures/gal.jpg\" width=800></img>\n",
    "</p>\n",
    "\n",
    "This morning's exercise will teach you:\n",
    "1. How to vectorize seismic traces from a scanned seismogram. \n",
    "2. How to convert the time series into ground motion using the transfer function\n",
    "3. Validate the spectra of the vectorised trace by comparing it with corresponding WaveWatch III ocean model\n",
    "\n",
    "## References:\n",
    "* Lecocq, T., Ardhuin, F., Collin, F., & Camelbeeck, T. (2020). On the extraction of microseismic ground motion from analog seismograms for the validation of ocean‐climate models. Seismological Research Letters, 91(3), 1518-1530.\n",
    "\n",
    "* Ishii, M., & Ishii, H. (2022). DigitSeis: software to extract time series from analogue seismograms. Progress in Earth and Planetary Science, 9(1), 50.\n",
    "\n",
    "* Tomasetto, L., Boué, P., Ardhuin, F., Stutzmann, É., Xu, Z., de Plaen, R., & Stehly, L. (2025, April). WMSAN: Wave Model Sources of Ambient Noise Python Library. From Modeling to Applications. In EGU General Assembly-Ambient Seismic Noise and Seismic Interferometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy import Trace, Stream, UTCDateTime\n",
    "from matplotlib.mlab import psd, magnitude_spectrum\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import feature\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.dates as mdatesAnalog seismograms on photographic paper, smoked paper and other magnetic support are records of ground motion that cover most of the 20th century. Whether for past earthquakes or the analysis of continuous wavefiled for time dependent seismology, this represents a wealth of information that can not be found anywhere else but remains hard to exploit with modern tools.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from skimage import data\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.transform import rotate, hough_line, hough_line_peaks\n",
    "import scipy.signal as ss\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import PIL.ImageOps  \n",
    "\n",
    "from obspy.signal.spectral_estimation import get_nlnm, get_nhnm\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['date.autoformatter.day'] = \"%Y-%m-%d\"\n",
    "mpl.rcParams['date.autoformatter.hour'] = \"%Y-%m-%d %Hh\"\n",
    "mpl.rcParams['date.autoformatter.minute'] = \"%Y-%m-%d %H:%M\"\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = \"12,8\"\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "\n",
    "mpl.rcParams['axes.axisbelow'] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "## Unsupervised vectorisation of continuous seismic records\n",
    "\n",
    "In the first exercise, we explored the vectorisation os seismic waveforms using [DigitSeis](references/digitseis_2022.pdf), a supervised digitisation tool. While it is efficient to solve complicated overlaping traces from earthquakes and other high implitude events, you also experienced how time consuming this approach can be. Since an increasing amount of modern methods rely on seismic ambient noise, an alternative approach is the extract the background traces and ingnore the complicated overlaping cases. While less comprehensive, this approach is also a lot faster and does not require supervision.\n",
    "\n",
    "For this exercises, we are reproducing the vectorization of the Galitzin recording of the \"Big Flood\",  a catastrophic flood caused by a heavy storm surge that severly impacted the Netherlands, Belgium and the UK in 1953, as detailed in [Lecocq et al. (2020)](references/lecocq_2020_srl.pdf)\n",
    ". The waveforms were recoded in Brussels and emphasize, the importance of studying the temporal variation of oceanic microseism in Legacy seismic data to better understand the Oceanic climate of the 20th century.\n",
    "\n",
    "| ![](./figures/flood_1953.jpg) | ![](./figures/Galitzin_horiz.jpg) |\n",
    "|-----------------|-----------------|\n",
    "| Aftermath of the \"Big Flood\" in the Dutch Islands | The horizontal Galitzin seismometer of The Royal Observatory of Belgium in Uccle |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the scanned file\n",
    "\n",
    "files = sorted(glob.glob(r\"./data/19530*.jpg\"))\n",
    "\n",
    "PLOT = True\n",
    "# Galitzin: bottom to up, left to right\n",
    "\n",
    "for file in files[1:]:\n",
    "    print(\"Processing %s\"%file)\n",
    "    filename = os.path.split(file)[1]\n",
    "    if filename in traces:\n",
    "        continue\n",
    "\n",
    "    im = Image.open(file)\n",
    "#     im = im.rotate(90)\n",
    "    im = PIL.ImageOps.invert(im)\n",
    "\n",
    "    im = np.asarray(im).copy()\n",
    "    print(im.shape)\n",
    "    if len(im.shape) > 2:\n",
    "        im = rgb2gray(im) \n",
    "\n",
    "    if PLOT:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.imshow(im, aspect='auto', interpolation=\"none\")\n",
    "        plt.savefig(\"figures/image_0_raw.jpg\")\n",
    "    # apply threshold\n",
    "    image = im.copy()\n",
    "    thresh = threshold_otsu(image)\n",
    "    bw = closing(image > thresh, square(3))\n",
    "    \n",
    "    if PLOT:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.imshow(bw, aspect='auto', interpolation=\"none\")\n",
    "        plt.savefig(\"figures/image_2_bw.jpg\")\n",
    "    \n",
    "    \n",
    "    # remove artifacts connected to image border\n",
    "    cleared = clear_border(bw)\n",
    "    \n",
    "    if PLOT:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.imshow(cleared, aspect='auto', interpolation=\"none\")\n",
    "        plt.savefig(\"figures/image_3_cleared.jpg\")\n",
    "    \n",
    "    angles = np.deg2rad(np.linspace(85,95,200))\n",
    "    hspace, angles, dists = hough_line(cleared, theta=angles)\n",
    "    hspace, angles, dists = hough_line_peaks(hspace, angles, dists)\n",
    "    angle = np.median(np.rad2deg(angles)) - 90\n",
    "    cleared = rotate(cleared, angle)\n",
    "    \n",
    "    if PLOT:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.imshow(cleared, aspect='auto', interpolation=\"none\")\n",
    "        plt.savefig(\"figures/image_4_rotated.jpg\")\n",
    "    \n",
    "    \n",
    "    print(cleared.shape)\n",
    "    # label image regions\n",
    "    label_image = label(cleared, connectivity=2)\n",
    "    print(label_image.shape)\n",
    "    image_label_overlay = label2rgb(label_image, image=cleared, bg_label=0, alpha=0.8)\n",
    "    if PLOT:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.imshow(image_label_overlay, aspect='auto', interpolation=\"none\")\n",
    "        plt.savefig(\"figures/image_5_label_overlay.jpg\")\n",
    "    i = 0\n",
    "    all = []\n",
    "    all_s = {}\n",
    "    all_box = []\n",
    "    all_id = {}\n",
    "    for region in regionprops(label_image):\n",
    "        # take regions with large enough area\n",
    "        if region.area >= 100:\n",
    "            # check zone size, if too small on time axis or too wide on amplitude, reject\n",
    "            _minr, _minc, _maxr, _maxc = region.bbox\n",
    "            if _maxc-_minc <= 50:\n",
    "                continue\n",
    "            if _maxr - _minr >= 100:\n",
    "                continue\n",
    "            # draw rectangle around segmented traces\n",
    "            minr, minc, maxr, maxc = region.bbox\n",
    "            if PLOT:\n",
    "                rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                                          fill=False, edgecolor='red', linewidth=0.1)\n",
    "                plt.text((maxc+minc)/2, minr, \"%05i\" %i, color='w', fontsize=2, horizontalalignment=\"center\")\n",
    "                ax.add_patch(rect)\n",
    "            i+=1\n",
    "            tmp = cleared[minr:maxr,minc:maxc]\n",
    "            tmp ^= tmp.min()\n",
    "            tmp[tmp>0] = 1.0\n",
    "            tmp = np.ascontiguousarray(tmp, dtype=np.uint8)\n",
    "            skeleton = skeletonize(tmp)\n",
    "            xx = []\n",
    "            for _ in range(skeleton.shape[1]):\n",
    "                _ = skeleton[:,_]\n",
    "                __ = np.where(_==1)[0]\n",
    "                if len(__):\n",
    "                    xx.append(__[0])\n",
    "                else:\n",
    "                    if len(xx):\n",
    "                        xx.append(xx[-1])\n",
    "                    else:\n",
    "                        xx.append(0)\n",
    "            _ = np.asarray(xx, dtype='float')\n",
    "            if _[0] == 0:\n",
    "                _[0] = _[1]\n",
    "\n",
    "            if PLOT:\n",
    "                plt.plot(np.arange(minc,maxc),minr+_, c='w', lw=0.1, zorder=100)\n",
    "            _ -= _.mean()\n",
    "            trace = {\"x\": minc, \"y\":(minr+maxr)/2., \"data\":_ }\n",
    "            all.append(trace)\n",
    "\n",
    "    traces[filename] = all\n",
    "    if PLOT:\n",
    "        ax.set_axis_off()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/image_6_skeleton.jpg\")\n",
    "#         plt.show()\n",
    "    del image, im, label_image, bw, cleared, image_label_overlay\n",
    "    break\n",
    "print(\"Done\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = list(traces.keys())[0].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The start time for each file/day has been collected from the scanned seismogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {}\n",
    "start_times[\"19530115.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530116.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530117.jpg\"] = \"07:22:00\"\n",
    "start_times[\"19530118.jpg\"] = \"09:40:00\"\n",
    "start_times[\"19530119.jpg\"] = \"06:59:00\"\n",
    "start_times[\"19530120.jpg\"] = \"07:02:00\"\n",
    "start_times[\"19530121.jpg\"] = \"07:52:00\"\n",
    "start_times[\"19530122.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530123.jpg\"] = \"07:25:00\"\n",
    "start_times[\"19530124.jpg\"] = \"07:08:00\"\n",
    "start_times[\"19530125.jpg\"] = \"09:50:00\"\n",
    "start_times[\"19530126.jpg\"] = \"07:10:00\"\n",
    "start_times[\"19530127.jpg\"] = \"07:08:00\"\n",
    "start_times[\"19530128.jpg\"] = \"07:45:00\"\n",
    "start_times[\"19530129.jpg\"] = \"07:43:00\"\n",
    "start_times[\"19530130.jpg\"] = \"07:25:00\"\n",
    "start_times[\"19530131.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530201.jpg\"] = \"09:45:00\"\n",
    "start_times[\"19530202.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530203.jpg\"] = \"07:19:00\"\n",
    "start_times[\"19530204.jpg\"] = \"07:54:00\"\n",
    "start_times[\"19530205.jpg\"] = \"07:58:00\"\n",
    "start_times[\"19530206.jpg\"] = \"08:00:00\"\n",
    "start_times[\"19530207.jpg\"] = \"07:30:00\"\n",
    "start_times[\"19530208.jpg\"] = \"09:14:00\"\n",
    "start_times[\"19530209.jpg\"] = \"09:27:00\"\n",
    "start_times[\"19530210.jpg\"] = \"07:47:00\"\n",
    "start_times[\"19530211.jpg\"] = \"08:08:00\"\n",
    "start_times[\"19530212.jpg\"] = \"07:00:00\"\n",
    "start_times[\"19530213.jpg\"] = \"08:15:00\"\n",
    "start_times[\"19530214.jpg\"] = \"08:00:00\"\n",
    "start_times[\"19530215.jpg\"] = \"10:14:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_traces = []\n",
    "\n",
    "\n",
    "for key in sorted(traces):\n",
    "    print(key)\n",
    "    date,_ = key.split('.')\n",
    "    d = datetime.datetime.strptime(date+ \" %s\" % start_times[key], \"%Y%m%d %H:%M:%S\")\n",
    "    df = pd.DataFrame(traces[key].copy())\n",
    "    df = df.sort_values([\"y\",\"x\"], ascending=[True,True])\n",
    "    ppmin = np.median([len(a) for a in df[\"data\"]])\n",
    "    print(\"ppmin\", ppmin)\n",
    "    if (ppmin < 300 or ppmin > 400) and key != \"19530118.jpg\":\n",
    "        ppmin=351\n",
    "    pps = ppmin / 59.0\n",
    "    timer = 0\n",
    "    for id, row in df.iterrows():\n",
    "        tr = Trace(data=row[\"data\"].copy())\n",
    "        tr.data *= 0.0254/300.0 # data is now in meters\n",
    "        if id == 10:\n",
    "            print(tr.data.ptp())\n",
    "        tr.stats.sampling_rate = pps\n",
    "        tr.stats.starttime = UTCDateTime(d) + timer\n",
    "        tr.detrend(\"linear\")\n",
    "        tr.interpolate(8, method='lanczos', a=64)\n",
    "        tr.taper(None, max_length=0.5, side=\"both\")\n",
    "        tr.filter(\"highpass\", freq=0.05, corners=8)\n",
    "        if id == 10:\n",
    "            print(tr.data.ptp())\n",
    "        processed_traces.append(tr)\n",
    "        timer += tr.stats.npts*tr.stats.delta + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = Stream(traces=processed_traces)\n",
    "st.write(\"{}.BHZ.mseed\".format(day))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waveform restitution\n",
    "\n",
    "The amplitude response is calculated from the constants collected from the 1953 seismic bulletin of the Royal Observatory of Belgium.\n",
    "The response plot highlight how the Galitzin instruments were particularly sensitive to the Primary and Secondary microseisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restitution_galitzin(ym, Tp=None, kind='horizontal', plot=False, show=True):\n",
    "    if Tp is None:\n",
    "        Tp = np.logspace(-1,2,num=200)\n",
    "    if kind == \"horizontal\":\n",
    "\n",
    "        T = 21.5\n",
    "        T1 = 21.8\n",
    "        l = 124.7\n",
    "        mu = 0.2\n",
    "        A1 = 1040.0\n",
    "        k = 38.0\n",
    "    \n",
    "    elif kind == \"vertical\":\n",
    "        T = 10.0\n",
    "        T1 = 10.15\n",
    "        l = 173.8 # somville 1937\n",
    "        mu = 0.0\n",
    "        A1 = 1060.0 #Somville 1930 bulletin\n",
    "        k = 290.0 # somville 1953  bulletin + correction for mistake in parameters leading to unrealistic magnitudes\n",
    "\n",
    "    else:\n",
    "        print(\"kind must be either horizontal or vertical, quitting\")\n",
    "        return False\n",
    "\n",
    "    u = Tp / T\n",
    "    u1 = Tp / T1\n",
    "\n",
    "    fu = (2.*u/(1.+u**2))**2\n",
    "    C1 = np.pi * l / (k*A1)\n",
    "    xm = C1 * ym/Tp * (1.+u**2)  * (1.+u1**2)  * np.sqrt(1.-fu*mu**2)\n",
    "    \n",
    "    if plot:\n",
    "        \n",
    "#         plt.title(\"ROB Galitzin Seismometer Amplitude Response (%s)\" % )\n",
    "        base, = plt.loglog(Tp,1./xm, lw=2)\n",
    "        maxx = np.argmax(1./xm)\n",
    "        plt.axvline(Tp[maxx], c=base.get_color(), ls=\"--\", \n",
    "                    label='%s Maximum: %.1f at %.1f s'%(kind.capitalize(), 1./xm[maxx], Tp[maxx]))\n",
    "        plt.grid(which='both')\n",
    "        plt.xlabel(\"Period (s)\")\n",
    "        plt.ylabel(\"Amplification\")\n",
    "        plt.legend(loc=2)\n",
    "        if show:\n",
    "            plt.show()\n",
    "    return xm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HORIZONTAL GALITZIN ROB\n",
    "ym = 1.0\n",
    "Tp = np.logspace(np.log10(0.2),2,num=200)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "xm = restitution_galitzin(ym, Tp, kind=\"horizontal\", plot=True, show=False)\n",
    "xm = restitution_galitzin(ym, Tp, kind=\"vertical\", plot=True, show=False)\n",
    "plt.grid(which=\"both\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/Amplitude_response_galitzin.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import read\n",
    "processed_traces=read('./data/1953.BHZ.mseed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_x = []\n",
    "all_pxx = []\n",
    "all_amp = []\n",
    "all_df2 = []\n",
    "all_times = []\n",
    "\n",
    "all_ptp = []\n",
    "all_domP = []\n",
    "\n",
    "for tr in processed_traces:\n",
    "    _ = tr.data.copy()\n",
    "    pxx, freqs = psd(tr.data, Fs=tr.stats.sampling_rate, pad_to=1024, detrend=\"mean\", noverlap=8, scale_by_freq=True)\n",
    "    all_ptp.append(np.max(np.abs(tr.data)))\n",
    "    maxP = np.min([12., 1./freqs[np.argmax(pxx)]])\n",
    "    amp =  restitution_galitzin(_, maxP, kind=\"vertical\", plot=False)\n",
    "    pxx, freqs = psd(amp, Fs=tr.stats.sampling_rate, pad_to=1024, detrend=\"mean\", noverlap=8, scale_by_freq=True)\n",
    "    all_domP.append(maxP)\n",
    "    all_x.append(freqs)\n",
    "    all_pxx.append(pxx)\n",
    "    freqs[0] = freqs[1]*0.1\n",
    "    all_amp.append(pxx)\n",
    "    all_times.append(tr.stats.starttime.datetime)\n",
    "df2 = pd.DataFrame(all_amp, columns=all_x[0], index=all_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp =  restitution_galitzin(np.ones(len(all_domP)),np.array(all_domP), kind=\"vertical\", plot=False)\n",
    "s = pd.Series(np.array(all_ptp)*amp*1e6, index=pd.DatetimeIndex(all_times))\n",
    "print(amp)\n",
    "plt.figure()\n",
    "plt.scatter(s.index, s )\n",
    "r = s.resample(\"30min\").quantile(0.95)\n",
    "plt.plot(r.index, r , c='blue')\n",
    "r = s.resample(\"30min\").mean()\n",
    "plt.plot(r.index, r , c='purple')\n",
    "plt.ylabel(\"Displacement, µm\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to match the ocean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean, tmean, scoreatpercentile\n",
    "df3 = df2.copy()\n",
    "df3 = df3.loc[:,1.0/18:]\n",
    "\n",
    "rs = df3.resample(\"3H\", axis=0)\n",
    "df3 = rs.quantile(0.95)\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "periods = 1. / df3.columns\n",
    "\n",
    "cmap = plt.get_cmap('inferno')\n",
    "cmap.set_bad(color = 'k', alpha = .6)\n",
    "\n",
    "# print(len(pmax), len(df3.index))\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "\n",
    "pmax = []\n",
    "for d in df3.values:\n",
    "    if np.any(np.isnan(d)):\n",
    "        pmax.append(np.nan)\n",
    "    else:\n",
    "        pmax.append(periods[np.argmax(d)])\n",
    "data = df3.values.T\n",
    "\n",
    "print(np.any(np.isnan(data)))\n",
    "plt.pcolormesh( df3.index.to_pydatetime().ravel(), periods, data, cmap=cmap, rasterized=True)\n",
    "plt.colorbar()\n",
    "plt.ylabel(\"Period (s)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylim(0.2,15)\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/1953.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Modelling Ocean\n",
    "\n",
    "The ocean model are generated using Wavewatch III and used specifically to simulate the corresponding oceanic microseisms. \n",
    "These sources are then propagated to the selected seismic station to simulate the corresponding ground motion. To explore more on this subject, see [Tomasetto et al. (2024)](references/tomasetto_2024.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4\n",
    "ncname = r\"./model/UCC_195301-195311_Q0200REF102040_edzf.nc\"\n",
    "\n",
    "nc = netCDF4.Dataset(ncname)\n",
    "edzf = nc.variables[\"edzf\"]\n",
    "print(edzf)\n",
    "print(edzf.units)\n",
    "times = nc.variables['time']\n",
    "freqsnc = nc.variables['frequency']\n",
    "jd = netCDF4.num2date(times[:],times.units)\n",
    "model = pd.DataFrame(edzf[:,0,:],index=jd, columns=freqsnc)\n",
    "model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the spectra of the digitized waveform with the ocean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.index = pd.to_datetime([t.strftime(\"%Y-%m-%d %H:%M:%S\") for t in model.index])\n",
    "day = model.loc[\"1953-01-15\":\"1953-02-15\"].copy()\n",
    "day = day.resample(\"3H\").median()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(211)\n",
    "periods = 1. / df3.columns\n",
    "\n",
    "cmap = plt.get_cmap('inferno')\n",
    "cmap.set_bad(color = 'k', alpha = .6)\n",
    "\n",
    "observed = df3.copy()\n",
    "\n",
    "pmax = []\n",
    "for d in observed.values:\n",
    "    if np.any(np.isnan(d)):\n",
    "        pmax.append(np.nan)\n",
    "    else:\n",
    "        pmax.append(periods[np.argmax(d)])\n",
    "\n",
    "# for id, row in observed.iterrows():\n",
    "#     observed.loc[id] /= ynew**3\n",
    "        \n",
    "data = 10* np.log10(observed.values).T\n",
    "# data = observed.values.T\n",
    "# data = np.sqrt(data)\n",
    "\n",
    "# print(len(observed.columns), data.shape, data.shape[1], ynew.shape)\n",
    "\n",
    "\n",
    "plt.pcolormesh( observed.index.to_pydatetime().ravel(), periods, data, cmap=cmap,  rasterized=True, vmax=-100, vmin=-140)\n",
    "cb = plt.colorbar(pad=0.01)\n",
    "cb.set_label('Decibel (dB)')\n",
    "\n",
    "# plt.semilogy(df3.index.to_pydatetime().ravel(), pmax, \"o\", c=\"w\")\n",
    "# plt.semilogy(df3.index[0], 1.0 )\n",
    "plt.ylim(1, 50)\n",
    "plt.ylabel(\"Period (s) | DPSD (dB)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylim(0.2,20)\n",
    "# fig.autofmt_xdate()\n",
    "plt.tight_layout()\n",
    "plt.title(\"Data\")\n",
    "plt.xlabel(\"\")\n",
    "plt.subplot(212, sharex=ax, sharey=ax)\n",
    "\n",
    "data = np.asarray(day)\n",
    "\n",
    "# for i,d in enumerate(data):\n",
    "#     data[i] *= (2*np.pi/day.columns)**2\n",
    "data = 10* np.log10(data).T\n",
    "# data *= 1e3\n",
    "# data = np.sqrt(data)\n",
    "plt.pcolormesh(day.index, 1./day.columns, data, cmap=cmap, vmax=-100, vmin=-140)\n",
    "cb = plt.colorbar(pad=0.01)\n",
    "cb.set_label('Decibel (dB)')\n",
    "\n",
    "\n",
    "# plt.semilogy(df3.index.to_pydatetime().ravel(), pmax, \"o\", c=\"w\")\n",
    "# plt.semilogy(df3.index[0], 1.0 )\n",
    "plt.ylabel(\"Period (s) | DPSD (dB)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.title(\"Model\")\n",
    "plt.ylim(2,12)\n",
    "# fig.autofmt_xdate()\n",
    "plt.xlabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(hspace=0.02)\n",
    "plt.savefig(\"1953_observed_vs_%s.pdf\"%os.path.split(ncname)[1])\n",
    "plt.savefig(\"1953observedvsmodel_spectrum.png\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "data = 10* np.log10(observed.values).T\n",
    "print(data.shape)\n",
    "\n",
    "plt.plot(periods, np.nanmean(data, axis=1), label=\"Jan-Feb 1953 - V - Data\")\n",
    "\n",
    "data = np.asarray(day)\n",
    "\n",
    "data = 10* np.log10(data).T\n",
    "\n",
    "plt.plot(1./day.columns, np.nanmean(data, axis=1), label=\"Jan-Feb 1953 - V - Model\")\n",
    "\n",
    "p, a = get_nlnm()\n",
    "plt.plot(p, a)\n",
    "p, a = get_nhnm()\n",
    "plt.semilogx(p, a)\n",
    "plt.legend()\n",
    "plt.xlim(0.5,50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = df3.copy()#.dropna(axis=\"index\", how=\"all\").dropna(axis=\"columns\", how='all')\n",
    "T2 = day.copy()#.dropna(axis=\"index\", how=\"all\").dropna(axis=\"columns\", how='all')\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "\n",
    "idx = T1.index.intersection(T2.index)\n",
    "\n",
    "tmp = T1.loc[idx].copy()\n",
    "tmp2 = T2.loc[idx].copy()\n",
    "\n",
    "# df = 1./8.0\n",
    "# print(df)\n",
    "# def psd_to_amp(a):\n",
    "#     return np.sqrt(a * df * 3)\n",
    "\n",
    "# tmp = tmp.apply(psd_to_amp)\n",
    "# tmp2 = tmp2.apply(psd_to_amp)\n",
    "\n",
    "def rms(a):\n",
    "    sel = a.loc[0.08:0.32]\n",
    "#     print(sel.index)\n",
    "#     sel -= sel.min()\n",
    "    return np.sqrt(np.trapz(sel.values, sel.index))*1.0e6\n",
    "\n",
    "test = tmp.apply(rms, axis=1)\n",
    "test2 = tmp2.apply(rms, axis=1)\n",
    "\n",
    "tmp = tmp.apply(np.sqrt)\n",
    "tmp2 = tmp2.apply(np.sqrt)\n",
    "\n",
    "\n",
    "conv = test2.max()/test.max()\n",
    "print(\"Conversion Factor: %f\"%conv)\n",
    "# print(conv, 1./conv)\n",
    "\n",
    "datamax = tmp.max(axis=1)\n",
    "def minmax(frame, dmax):\n",
    "    \n",
    "    low = []\n",
    "    high= []\n",
    "    \n",
    "    idx = frame[frame>=dmax]\n",
    "    return idx\n",
    "        \n",
    "datamax95 = tmp.apply(minmax, dmax=datamax*0.70)\n",
    "\n",
    "\n",
    "sns.set_palette(\"Dark2\")\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "ax1 = plt.subplot(211)\n",
    "# plt.plot(tmp.index, tmp.max(axis=1)*conv, label=\"data * %g\"%conv)\n",
    "# plt.plot(tmp2.index, tmp2.max(axis=1), c='r', label=\"model\")\n",
    "\n",
    "plt.plot(test.index, test, '-+', label=\"data\", markersize=5)\n",
    "# plt.plot(test.index, test*conv, '-+', label=\"data scaled\", markersize=5)\n",
    "plt.plot(test2.index, test2, label=\"model\")\n",
    "\n",
    "dr = pd.date_range(test.index[0].date(), test.index[-1].date(), freq=\"D\")\n",
    "drl = [d.strftime(\"%Y-%m-%d\") for d in dr]\n",
    "plt.xticks(dr, drl)\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.ylabel(\"$\\delta_{rms}$ (µm)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.twinx()\n",
    "# localsource = pd.read_csv(\"localsources.csv\", index_col=0, parse_dates=True).loc[\"1953-01-15\":\"1953-02-15\"]\n",
    "# plt.plot(localsource.index, localsource, c='k')\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "# a = 1./tmp2.idxmax(axis=1)\n",
    "# b = 1./tmp.idxmax(axis=1)\n",
    "\n",
    "# plt.plot([3.,10.], [3.,10.], c=\"r\")\n",
    "# plt.xlim(3,10)\n",
    "# plt.ylim(3,10)\n",
    "\n",
    "# sns.regplot(x=a, y=b)\n",
    "\n",
    "\n",
    "# plt.xticks(np.arange(3,11),np.arange(3,11), zorder=-10)\n",
    "# plt.yticks(np.arange(3,11),np.arange(3,11), zorder=-10)\n",
    "# plt.xlabel(\"Model (s)\")\n",
    "# plt.ylabel(\"Data (s)\")\n",
    "# plt.title(\"Max Peak Period\")\n",
    "\n",
    "\n",
    "ax3 = plt.subplot(212, sharex=ax1)\n",
    "datamax = tmp.idxmax(axis=1)\n",
    "plt.plot(tmp.index, 1./datamax, \"-+\", label=\"data\", markersize=5)\n",
    "plt.xticks(dr, drl)\n",
    "indexes = []\n",
    "mins = []\n",
    "maxs = []\n",
    "for index, row in datamax95.iterrows():\n",
    "    if np.all(np.isnan(row)):\n",
    "        continue\n",
    "    else:\n",
    "        row.dropna(inplace=True)\n",
    "        indexes.append(index)\n",
    "        mins.append(1./row.first_valid_index())\n",
    "        maxs.append(1./row.last_valid_index())\n",
    "\n",
    "plt.fill_between(indexes, mins, maxs, zorder=-1, facecolor='lightblue', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.plot(tmp2.index, 1./tmp2.idxmax(axis=1), label=\"model\")\n",
    "\n",
    "plt.xlim(tmp2.index[0], tmp2.index[-1])\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.ylim(3,10)\n",
    "plt.ylabel(\"Period of PSD max value (s)\")\n",
    "plt.grid(True)\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "\n",
    "a = pd.concat((test, test2), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax2 = plt.axes([0,0,1,1], aspect='equal', zorder=100)\n",
    "ip = InsetPosition(ax1, [0.01,0.60,0.35, 0.35])\n",
    "ax2.set_axes_locator(ip)\n",
    "plt.sca(ax2)\n",
    "plt.scatter(a[0], a[1], s=2, c='k')\n",
    "plt.plot([0,2], [0,2],c='k')\n",
    "sns.regplot(x=a[0], y=a[1], ax=ax2, scatter=False, label=False)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('model')\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(0,2)\n",
    "\n",
    "\n",
    "\n",
    "ax4 = plt.axes([0,0,.99,.99], aspect='equal', zorder=100)\n",
    "ip2 = InsetPosition(ax3, [0.01,0.60,0.35, 0.35])\n",
    "ax4.set_axes_locator(ip2)\n",
    "plt.sca(ax4)\n",
    "a = 1./tmp.idxmax(axis=1)\n",
    "b = 1./tmp2.idxmax(axis=1)\n",
    "plt.scatter(a, b, s=2, c='k')\n",
    "plt.plot([0,9], [0,9],c='k')\n",
    "sns.regplot(x=a, y=a, ax=ax4, scatter=False, label=False)\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('model')\n",
    "plt.xlim(4,9)\n",
    "plt.ylim(4,9)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.02)\n",
    "plt.tight_layout()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(\"maxvalueandperiod.pdf\")\n",
    "plt.savefig(\"figures/1953observedvsmodel_valper_Z.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
